![](https://wikidocs.net/images/page/228835/autoregressive1.png)

앞선 챕터에서 살펴보았듯이, 생성 모델의 목적은 주어진 데이터 $x$에 대한 확률 분포 $p(x)$를 찾는 데 있습니다. 만일 강아지의 이미지가 있는 데이터셋을 가지고 생성 모델을 학습한다면 다음과 같은 성질들을 만족해야 합니다.

* Generation: 추정한 $p(x)$에서 새로운 표본 $x_{new}$을 추출한다면, 해당 표본은 강아지와 유사해야 합니다.
* Representation Learning: 강아지 데이터셋 전반적으로 나타나는 특징을 학습해야 합니다.
> ears, nose, tail, etc.
* Density Estimation: 주어진 데이터 $x$가 강아지와 유사하면 높은 $p(x)$을 가져야 하고, 강아지가 아닌 오차 이미지를 입력하면 낮은 $p(x)$을 가져야 합니다. (이상 탐지(Anomaly Detection))

자가회귀모형(AutoRegressive)은 $p(x)$을 직접적으로(explicitly) 나타낼 수 있는(tractable)한 생성 모델 알고리즘으로, $i$ 번째 시간대의 확률 분포 $p(x_i)$를, 해당 시간대 이전 $<i$, $(1, 2, \cdot\cdot\cdot, i-1)$ 까지의 데이터 $x_{<i}$, $(x_1, x_2, \cdot\cdot\cdot, x_{i-1})$을 가지고 예측합니다.

$p(x)$의 분포를 직접적으로 나타낼 수 있는 경우, 크게 세가지 방법으로 $p(x)$을 나타낼 수 있습니다.

## 1. 연쇄 법칙 (Chain Rule)

$$p(x_1, x_2, x_3, x_4) = p(x_1)p(x_2 | x_1)p(x_3 | x_1, x_2)p(x_4 | x_1, x_2, x_3)$$

연쇄법칙을 사용하면 $p(X)$의 분포 $p(x_1, x_2, x_3, x_4)$를 추가적인 가정 없이 나타낼 수 있다는 장점이 있지만, 데이터 수가 증가할 수록 계산해야 할 파라미터가 기하급수적으로 늘어난다는 단점이 있습니다.

만일 각각의 요소 $x_i$가 $0$ 혹은 $1$을 출력하는 베루누이 확률 분포를 따른다면, $(x_i \in [0,1])$

$p(x_1)$은 $0$ 혹은 $1$을 판별하는 1개의 파라미터가 필요하지만,<br>
$p(x_1, x_2)$은 $p(x_2|x_1 = 0)$, $p(x_2|x_1 = 1)$ 2개의 파라미터가 필요하고,<br>
$p(x_1, x_2, x_3)$은 $p(x_3|x_1 = 0, x_2 = 0)$, $p(x_3|x_1 = 1, x_2 = 0)$, $p(x_3|x_1 = 0, x_2 = 1)$, $p(x_3|x_1 = 1, x_2 = 1)$ 4개의 파라미터를 요구합니다.

즉 요소의 수가 $n$개라면 총 파라미터 수는 $1 + 2 + 4 + \cdot\cdot\cdot + 2^{n-1} = 2^n - 1$이 되고, Big-O notation에 따라 $2^n$에 수렴하므로 엄청나게 큰 컴퓨팅 파워를 요구합니다.

## 2. 베이즈 네트워크 (Bayesian Network)

$$p(x_1, x_2, x_3, x_4) ≈ p_{CPT}(x_1)p_{CPT}(x_2 | x_1)p_{CPT}(x_3 | \cancel{x_1}, x_2)p_{CPT}(x_4 | \cancel{x_1}, \cancel{x_2}, x_3)$$

$p(X|Y,Z) = p(X|Z)$을 따르는 베이즈 정리의 조건부 독립가정 (conditional independence assumptions)를 따른다면, 요소 $x_i$의 분포를 예측하기 위해 $<i$ 시간대의 요소 전부를 사용할 필요 없이 직전의 요소 $x_{i-1}$의 요소만을 고려한 Compact (CPT)한 가정을 수행할 수 있습니다. 

베이즈 네트워크를 사용한다면 파라미터 수를 $2^n -1$개에서 $2n-1$개로 줄일 수 있고, 모델의 용량은 Big-O notation에 따라 $n$에 수렴하므로 아주 컴팩트한 모델을 구축할 수 있지만, 직전의 요소만을 고려해 분포를 생성하기 때문에 심층적인 정보를 담으려면 추가적인 가정이 필요합니다.

## 3. 심층 신경망 구조 (Parametric Nueral Models)

$$p(x_1, x_2, x_3, x_4) ≈ p(x_1)p(x_2 | x_1)p_{Neural}(x_3 | x_1, x_2)p_{Neural}(x_4 | x_1, x_2, x_3)$$

마지막으로, $p(X)$의 분포 $p(x_1, x_2, x_3, x_4)$를 계산하기 위해 많은 컴퓨팅 파워를 요구하는 조건부 확률 $p(x_i|x_{<i})$를 매개변수화 (parameterize)하여 심층 신경망 구조 (deep neural networks)에 적용해 모델을 학습함으로서 $p(x_4 | x_1, x_2, x_3)$를 $p_{Neural}(x_4 | x_1, x_2, x_3)$으로 근사할 수 있습니다. 이 경우 파라미터 수를 절약하면서도 강력한 생성 모델을 구축할 수 있고, 저희가 앞으로 다루고자 하는 AutoRegressive Models가 이에 해당합니다.

## 참고자료

[Cornell CS 6875 Deep Generative Models. Lecture 3: Autoregressive Models](https://www.youtube.com/watch?v=Y3cJFaM8w2w)<br>
생성모델 (STA4124.01-00) week5_2, week6_1, week6_2, 송경우 교수님